import { json } from '@sveltejs/kit';
import Together from 'together-ai';
import { config } from '$lib/config';

const together = new Together({ apiKey: config.TOGETHER_AI_API_KEY });

export async function POST({ request }: { request: Request }) {
  try {
    const { text }: { text: string } = await request.json();

    const prompt: string = `Extract multiple-choice questions from the following text and return only JSON.
Do not include explanations, comments, or examples. Strictly follow this structure:

{
  "questions": [
    {
      "ë¬¸ì œ": "ì‹¤ì œ ë³€í™˜ëœ ì§ˆë¬¸ ë‚´ìš©",
      "ì´ë¯¸ì§€": "",
      "ì„ íƒì§€": [
        { "ì„ íƒì§€": "ì‹¤ì œ ë³€í™˜ëœ ì„ íƒì§€1", "ì´ë¯¸ì§€": "" },
        { "ì„ íƒì§€": "ì‹¤ì œ ë³€í™˜ëœ ì„ íƒì§€2", "ì´ë¯¸ì§€": "" },
        { "ì„ íƒì§€": "ì‹¤ì œ ë³€í™˜ëœ ì„ íƒì§€3", "ì´ë¯¸ì§€": "" },
        { "ì„ íƒì§€": "ì‹¤ì œ ë³€í™˜ëœ ì„ íƒì§€4", "ì´ë¯¸ì§€": "" }
      ],
      "ì •ë‹µ": ì‹¤ì œ ì •ë‹µ ì¸ë±ìŠ¤ ìˆ«ìë¡œ
    }
  ]
}

Now extract the questions from the following text:
${text}`;

    const response = await together.chat.completions.create({
      model: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo-128K", // ğŸ¦™ Llama 3.1 8B ëª¨ë¸ ì ìš©
      messages: [
        { role: "system", content: "Extract multiple-choice questions from the given text and return them in valid JSON format." },
        { role: "user", content: prompt }
      ],
      max_tokens: null,
      temperature: 0.7,
      top_p: 0.7,
      top_k: 50,
      repetition_penalty: 1,
      stop: ["<|eot_id|>", "<|eom_id|>"],
      stream: false // âŒ ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™” (ì›í•˜ëŠ” ê²½ìš° trueë¡œ ë³€ê²½ ê°€ëŠ¥)
    });

    console.log("Together AI ì‘ë‹µ:", response.choices[0].message.content);
    return json(JSON.parse(response.choices[0].message.content)); // JSON ë³€í™˜
  } catch (error: any) {
    return json({ error: 'API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ', details: error.message }, { status: 500 });
  }
}
